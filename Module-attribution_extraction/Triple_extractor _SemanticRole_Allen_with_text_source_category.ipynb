{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:57:21.234256Z",
     "start_time": "2020-07-23T16:57:21.223957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:85% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<style>.container { width:85% !important; }</style>'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare sip list\n",
    "Source indicating predicates are a list of verbs typically used to indicate attribution predicates.\n",
    "can be saved as external file (.txt would work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:57:23.974128Z",
     "start_time": "2020-07-23T16:57:23.971057Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:57:24.349919Z",
     "start_time": "2020-07-23T16:57:24.334114Z"
    }
   },
   "outputs": [],
   "source": [
    "# These are the SIPs we will use to filter out the predicates in the main program\n",
    "with open ('./data/sip.txt', 'r') as infile:\n",
    "    split_these = infile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:57:24.837963Z",
     "start_time": "2020-07-23T16:57:24.834733Z"
    }
   },
   "outputs": [],
   "source": [
    "# here we split the SIP list into the verb (lemma) plus its frequency in the parc corpus\n",
    "import re\n",
    "\n",
    "sip_splitter = re.compile('\\w+\\s\\d+')\n",
    "sips_to_sort = sip_splitter.findall(split_these)\n",
    "\n",
    "# the least frequent verbs are removed (fequency = <=10)\n",
    "filtered_sips = [items for items in sips_to_sort if int(items[-2:])>10]\n",
    "filtered_sips_alone =[sips.strip('0123456789').strip() for sips in filtered_sips]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract all triples (subject predicate object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:57:25.995205Z",
     "start_time": "2020-07-23T16:57:25.980658Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def extract_subj_pred_obj (file):\n",
    "    '''\n",
    "    takes filename as input (conll format) and returns a list of every subject predicate & object triple \n",
    "    in the text based on the allen semantic role labelling system (ARG0, B-V, ARG1)\n",
    "    '''\n",
    "    #read in conll file\n",
    "    with open (file, 'r', encoding = 'utf-8') as infile:\n",
    "        text = infile.read()\n",
    "    text_rows = text.splitlines()\n",
    "    rows = [row.split('\\t') for row in text_rows]\n",
    "    \n",
    "    # to iterate through each sentence the for loop should be as long as the number of sentences in the file\n",
    "    # as indicated by the last row [-1], first index position [0].\n",
    "    \n",
    "    num_sents = int(rows[-1][0])\n",
    "    # skip header\n",
    "    rows_only = rows[1:]\n",
    "    \n",
    "    file_tuples_list = []\n",
    "    \n",
    "    for n in range(1, num_sents):\n",
    "        sent_id = n\n",
    "        col_subj = set()\n",
    "        col_obj = set()\n",
    "        col_pred = set()\n",
    "    \n",
    "        for row in rows_only:\n",
    "            # for a particular sentence_id\n",
    "            if row[0] == str(sent_id):\n",
    "                # get the index of each item in each row in that sentence\n",
    "                for i, item in enumerate(row):\n",
    "\n",
    "                    if 'ARG0' in item: \n",
    "                        col_subj.add(i)\n",
    "                    elif 'ARG1' in item:\n",
    "                        col_obj.add(i)\n",
    "                    elif item == 'B-V':\n",
    "                        col_pred.add (i)\n",
    "\n",
    "        # since the index of a row is the column position in the file, \n",
    "        # if a col (i) has a subj, pred and obj in a given sentence span, we want it:\n",
    "        # we can find the intersection (shared numbers) in the above sets and keep the overlapping indexes\n",
    "        \n",
    "        search_col = set.intersection(col_subj, col_pred, col_obj)\n",
    "        \n",
    "        if len(search_col)>0:\n",
    "            # the value in search col is the index_number of the column we want in each row\n",
    "            for value in search_col:\n",
    "                subj_phrase= ''\n",
    "                obj_phrase = ''\n",
    "                pred=''\n",
    "                # we want to cycle through the rows again now we know which columns we are interested in\n",
    "                for row in rows_only:\n",
    "                    try:\n",
    "                        if row[0] == str(sent_id):\n",
    "                            column = row[value] \n",
    "                            word = row[2] \n",
    "                            lemma = row[3]\n",
    "                            # we extract the id of the predicate to evaluate (compare with human annotations)\n",
    "                            index = rows.index(row)\n",
    "                            if 'ARG0' in column:\n",
    "                                subj_phrase+=' '+word\n",
    "                            elif 'ARG1' in column:\n",
    "                                obj_phrase+=' '+word\n",
    "                            elif column == 'B-V':\n",
    "                                ind = index+1\n",
    "                                pred = lemma #we use the lemma here instead of the word to filter on later\n",
    "                                \n",
    "                    except:\n",
    "                        pass\n",
    "#                         print('some isssue with', file)\n",
    "                #once extracted, a tuple is created for each column (this may be multiple for embedded phrases)\n",
    "                one_shot = (subj_phrase, pred, obj_phrase, ind)\n",
    "                file_tuples_list.append(one_shot)\n",
    "\n",
    "    return file_tuples_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean tuples and creates output dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the category of the text source to add to the output dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:57:30.920254Z",
     "start_time": "2020-07-23T16:57:29.602992Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# text_source_category.csv is a list of the publishers and their respective classification (news, journal)\n",
    "\n",
    "with open ('./data/text_source_category.csv', 'r') as infile:\n",
    "    text_source_classes_text = infile.readlines()\n",
    "# convert text to list of 2 items (publisher | text source class)   \n",
    "publishers_type_list = [item.strip().split(';') for item in text_source_classes_text[1:]]\n",
    "\n",
    "# some cleaning - stripping \\n from the second element\n",
    "publishers_type_list[0][0]=publishers_type_list[0][0].strip(',')\n",
    "# convert list to dict to perform mapping in dataframe\n",
    "publishers_type_dict = {key:value for key, value in publishers_type_list}\n",
    "# look up which filename is associated with which publisher (info in tsv file)\n",
    "df = pd.read_csv('./data/metadata.tsv', sep ='\\t')\n",
    "# create new column in dataframe where col 'type' is determined by the col 'publisher' \n",
    "df['type']=df.Publisher.map(publishers_type_dict)\n",
    "\n",
    "# this resulting df can be used to lookup text-class based on publisher info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:57:31.764332Z",
     "start_time": "2020-07-23T16:57:31.759132Z"
    }
   },
   "outputs": [],
   "source": [
    "def source_type (stripped_basename):\n",
    "    '''\n",
    "    takes a stripped basename of a file, searches a datafrane for the filename; looks up the text source \n",
    "    category and returns the appropriate classification to be appended to a dictionary value list \n",
    "    '''\n",
    "    source_class = df.loc[df['File_ID'] == stripped_basename, 'type'].values[0]\n",
    "    \n",
    "    return source_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## creating the output dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:59:27.514066Z",
     "start_time": "2020-07-23T16:58:27.889631Z"
    }
   },
   "outputs": [],
   "source": [
    "#  change for relative file path to conll-allen-nlp\n",
    "allen_nlp_directory = '../conll-allen-nlp'\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# final output is dictionary: key is basename of file; values are SIP filtered triples from that file \n",
    "triple_tuple_dict = {}\n",
    "dict_of_SIP_indexes={}\n",
    "\n",
    "for filename in glob.glob(f'{allen_nlp_directory}/*'):\n",
    "    # the stripped basename of the file will be used twice below (as dict_key)\n",
    "    dict_key = os.path.splitext(os.path.basename(filename))[0]\n",
    "    # we identify the text source class\n",
    "    try:\n",
    "        text_class = source_type(dict_key)\n",
    "    except:\n",
    "#         print(filename)\n",
    "        # two files are causing issues - @berkleywellness & Science-_-AAS - both are news so forced the class below\n",
    "        text_class = 'news'\n",
    "    # here we execute the above function\n",
    "    triple_list = extract_subj_pred_obj(filename)\n",
    "    cleaned_triples = [triple for triple in triple_list if len(triple)>0]\n",
    "    sip_filtered_triple = []\n",
    "    # this isolates the index of the SIP\n",
    "    ind_alone=[]\n",
    "    for triple in cleaned_triples:\n",
    "        if triple[1] in filtered_sips_alone:\n",
    "            sip_filtered_triple.append(triple)\n",
    "            ind_alone.append(triple[-1])\n",
    "    sip_filtered_triple.append(text_class)\n",
    "    \n",
    "    triple_tuple_dict[dict_key]=sip_filtered_triple\n",
    "    \n",
    "    #dict of basename: list of indeces for SIPs in basename file\n",
    "    dict_of_SIP_indexes[dict_key]=ind_alone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# accessing output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:59:42.146759Z",
     "start_time": "2020-07-23T16:59:42.117115Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample category look-up\n",
    "triple_tuple_dict['Activist-Post_20170704T090503'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T16:59:51.611423Z",
     "start_time": "2020-07-23T16:59:51.592189Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a failing measles vaccine is behind the outbreak \n",
      "\n",
      " the marketing and cheerleading arm of the vaccine industry and the medical-industrial complex \n",
      "\n",
      " an article titled , “ The 2013 Measles Outbreak : A Failing Vaccine , Not A Failure To Vaccinate , ” which deconstructed the myth that the minimally – or non-vaccinated were responsible for outbreaks of measles in highly vaccination-compliant populations \n",
      "\n",
      " to subject themselves to them \n",
      "\n",
      " this \n",
      "\n",
      " We conclude that outbreaks of measles can occur in secondary schools , even when more than 99 percent of the students have been vaccinated and more than 95 percent are immune \n",
      "\n",
      " that outbreaks of measles can occur in secondary schools , even when more than 99 percent of the students have been vaccinated and more than 95 percent are immune \n",
      "\n",
      " “ This outbreak suggests that measles transmission may persist in some settings despite appropriate implementation of the current measles elimination strategy \n",
      "\n",
      " that measles transmission may persist in some settings despite appropriate implementation of the current measles elimination strategy \n",
      "\n",
      " that 98.7 % of students were appropriately vaccinated \n",
      "\n",
      " … measles outbreaks can occur among highly vaccinated college populations \n",
      "\n",
      " Incomplete vaccination coverage is not a valid explanation for the Quebec City measles outbreak .4 1991-1992 , Rio de Janeiro , Brazil : According to an article published in the journal Revista da Sociedade Brasileira de Medicina Tropical , in a measles outbreak from March 1991 to April 1992 in Rio de Janeiro , 76.4 % of those suspected to be infected had received measles vaccine before their first birthday .5 1992 , Cape Town , South Africa : According to an article published in the South African Medical Journal in 1994 , “ [ In ] August 1992 an outbreak occurred , with cases reported at many schools in children presumably immunised \n",
      "\n",
      " that primary and secondary vaccine failure was a possible explanation for the outbreak .6 These six outbreaks are by no means exhaustive of the biomedical literature \n",
      "\n",
      " susceptibility \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample attribution content extraction (the [:-1] avoids the non-triple - text_source category at the end of the list)\n",
    "\n",
    "for triples in triple_tuple_dict['Activist-Post_20170704T090503'][:-1]:\n",
    "    print (triples[2], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Output\n",
    "\n",
    "- the precision, recall and f1 scores are evaluated by comparing the indexes of the B-Cue in gold (allen.conll) and the system identified B-V semantic role label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T09:04:56.659144Z",
     "start_time": "2020-07-25T09:04:56.656141Z"
    }
   },
   "outputs": [],
   "source": [
    "# dict of indexes of cues (values) per file (key)\n",
    "dict_of_SIP_indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T17:10:31.347798Z",
     "start_time": "2020-07-23T17:10:30.424855Z"
    }
   },
   "outputs": [],
   "source": [
    "# create list of file-name + list of indexes of attr_cue\n",
    "allen_nlp_directory = '../conll-allen-nlp'\n",
    "gold_attr_index_dict = {}\n",
    "for filename in glob.glob(f'{allen_nlp_directory}/*'):\n",
    "    dict_key = os.path.splitext(os.path.basename(filename))[0]\n",
    "    with open (filename, 'r', encoding = 'utf-8') as infile:\n",
    "        text = infile.read()\n",
    "    output_list =[]\n",
    "    text_rows = text.splitlines()\n",
    "    rows = [row.split('\\t') for row in text_rows]\n",
    "    for row in rows:\n",
    "        #some lines are blank\n",
    "        try:\n",
    "            if 'B-cue' in row[12]:\n",
    "                index = rows.index(row)\n",
    "                output_list.append(index+1)\n",
    "                \n",
    "        except:\n",
    "            pass\n",
    "    gold_attr_index_dict[dict_key]= output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T09:05:28.131911Z",
     "start_time": "2020-07-25T09:05:28.128909Z"
    }
   },
   "outputs": [],
   "source": [
    "gold_attr_index_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T10:37:19.644343Z",
     "start_time": "2020-01-31T10:37:19.597465Z"
    }
   },
   "outputs": [],
   "source": [
    "# matching == True Positives(tp); in dict, but not in conll = False Positives(fp); in conll not in dict = False Negatives(fn).\n",
    "# this provides file by file stats for potential bug-hunting\n",
    "matching = {}\n",
    "# these are aggregates scores for final analysis\n",
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "\n",
    "for key, values in dict_of_SIP_indexes.items():\n",
    "    for gold_key, gold_values in gold_attr_index_dict.items():\n",
    "        matching_list = 0\n",
    "        if gold_key == key:\n",
    "            len_gold = len(gold_values)\n",
    "            len_system = len(values)\n",
    "            for value in values:\n",
    "                # if system in gold\n",
    "                if value in gold_values:\n",
    "                    matching_list+=1\n",
    "                else:\n",
    "                    pass\n",
    "            TP = matching_list\n",
    "            FP = len_system-matching_list\n",
    "            FN = len_gold - matching_list\n",
    "            \n",
    "            tp+=TP\n",
    "            fp+=FP\n",
    "            fn+=FN\n",
    "            # cannot divide by 0\n",
    "            if TP >0:\n",
    "                precision = TP/(TP+FP)\n",
    "                recall = TP/(TP+FN)\n",
    "            else:\n",
    "                precision = 'div by zero'\n",
    "                recall = 'div by zero'\n",
    "            inner_dict = {}\n",
    "            inner_dict['precision']=precision\n",
    "            inner_dict['recall'] = recall\n",
    "            matching[key]= inner_dict     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T09:05:34.760590Z",
     "start_time": "2020-07-25T09:05:34.758597Z"
    }
   },
   "outputs": [],
   "source": [
    "matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T10:37:54.917361Z",
     "start_time": "2020-01-31T10:37:54.901735Z"
    }
   },
   "outputs": [],
   "source": [
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "f1 = 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T10:38:03.636778Z",
     "start_time": "2020-01-31T10:38:03.621151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6424983620877921"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T10:38:09.371592Z",
     "start_time": "2020-01-31T10:38:09.355964Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46207004868855034"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5375479627261099"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
